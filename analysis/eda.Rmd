---
title: "EDA"
output: html_document
---

<style type="text/css">
  body{
  font-size: 16pt;
}
</style>

```{r include=FALSE}

knitr::opts_chunk$set(echo = FALSE, error = FALSE, warning = FALSE,
                      message = FALSE)

```



```{r load_libraries}

devtools::load_all()

library(tidyverse)

library(tidymodels)

library(corrr)

library(ggcorrplot)

library(glue)

library(ggmosaic)



```

```{r load_data}

data(train_set)



train_set = train_set %>% 
  rename("id" = 1) %>% 
  rename(target = "lnwage")

# data(test_set)
# 
# test_set = test_set %>% 
#   rename("id" = 1)

```

```{r set_params}

professions = c(
  "manager",
  "business",
  "financialop",
  "computer",
  "architect",
  "scientist",
  "socialworker",
  "postseceduc",
  "legaleduc",
  "artist",
  "lawyerphysician",
  "healthcare",
  "healthsupport",
  "protective",
  "foodcare",
  "building",
  "sales",
  "officeadmin",
  "farmer",
  "constructextractinstall",
  "production",
  "transport"
)


```


```{r summarise_data, eval=FALSE}

train_set %>% 
  skimr::skim()

```

```{r convert_to factors}

train_set = train_set %>% 
  map_dfc(function(temp_col){
    
  if(length(unique(temp_col)) <= 2){
    return(as.factor(temp_col))
    }else {return(as.numeric(temp_col))}
  })

```

The dataset have 38 features, most of them categorical binary features. There are `r sum(map_lgl(train_set, is.factor))` factor features and `r sum(map_lgl(train_set, is.numeric)) - 1` numeric features.

### Outliers

```{r outliers}

train_set %>% 
  select(where(is.numeric),-id,-target) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(y = value)) + 
  geom_boxplot() + 
  facet_wrap(~name, scales = "free")

```

We can see that there are outliers in all the numeric features.
For the squared features I'll try the Yeo-Johnson transformation to deal
with skewness.

```{r yeo_johnson_transformation}

test_recipe = recipe(target ~ ., train_set) %>% 
  step_YeoJohnson(ends_with("sq")) %>%
  identity()

train_set = test_recipe %>% 
  prep() %>% 
  bake(train_set)


train_set %>% 
  select(ends_with("sq")) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(y = value)) + 
  geom_boxplot() + 
  facet_wrap(~name, scales = "free")


rm(test_recipe)

```


## Numeric features

The numeric features are : `r train_set %>% select(where(is.numeric)) %>% names() %>%  glue_collapse(sep = ",")`. When we look at the distribution of the
features we can see that experience (and especially squared experience) is quite
skewed with right tail outliers.There is a question of whether this is "a feature or a bug" since the squaring was introduced to catch the nonlinearity in experience. 

```{r dist_of_numeric_features}

train_set %>% 
  select(where(is.numeric)) %>% 
  pivot_longer(cols = -id) %>% 
  ggplot(aes(x = value)) + 
  geom_histogram() + 
  facet_wrap(~name, scales = "free") + 
  xlab(NULL) + ylab(NULL)
  

```

When looking at the correlations of the numeric features with target we see that education and father experience are positively correlated with target (the square father experience has low correlation despite the fact that target has only positive values). The strange result is the negative correlation between experience (and its square) to target.

```{r correlations_of_numeric_features}

train_set %>% 
  select(-id) %>% 
  select(where(is.numeric)) %>% 
  correlate() %>% 
  stretch() %>% 
  rename(feature = y) %>% 
  filter(complete.cases(.)) %>% 
  filter(x == "target") %>% 
  ggplot(aes(x = reorder(feature,r), y = r)) + 
  geom_col() + 
  coord_flip() + 
  xlab(NULL) + ylab(NULL) + 
  ggtitle("Correlations of numeric features with target")

```

```{r scatterplot_of_numeric_features}

train_set %>% 
  select(where(is.numeric), target,-id) %>% 
  pivot_longer(-target) %>% 
  ggplot(aes(x = value, y = target)) + 
  geom_point() + 
  geom_smooth(method = "glm") + 
  facet_wrap(~name, scales = "free") + 
  xlab(NULL) + ylab(NULL) + ggtitle("Target distribution by numeric features")

```

Does experience and education years (after 18 years old)sum up to age?


```{r age_distribution}

train_set %>% 
  mutate(age = expp + edyrs + 18) %>% 
  ggplot(aes(x = age)) + 
  geom_histogram() + 
  xlab(NULL) + ylab(NULL) + ggtitle("Age (expp + edyrs + 18) distribution")

```


## Categorical features

There are many features that are the results of one hot encoding of the categorical features. Perhaps we can try another encoding and so I collect back the categories to
one categorical variable


```{r boxplot_by_target, fig.height=7}

train_set %>% 
  select(where(is.factor), target) %>% 
  pivot_longer(-target) %>% 
  ggplot(aes(x = value, y = target)) + 
  geom_boxplot() + 
  facet_wrap(~name, scales = "free") + 
  xlab(NULL) + ylab(NULL) + ggtitle("Target distribution by cat features")



```


```{r inverse_ohe}

train_set = train_set %>% 
  pivot_longer(cols = all_of(professions),names_to = "profession") %>% 
  filter(value == 1) %>% 
  select(-value)

```


Let's see what are the mean values of target by profession, in general the data looks good (computer, business and finance making more than social worker and building) but there are "outliers" such as artist (at the top) or scientist (at the middle).Checking out the distribution of target by profession we can see that
there is one rich artist that stands out. One way of dealing with it is to try
to encode with median target by profession


```{r average_target_by_profession}


train_set %>% 
  select(target, profession) %>% 
  group_by(profession) %>% 
  summarise(mean_target = mean(target), .groups = "drop") %>% 
  ggplot(aes(x = reorder(profession,mean_target), y = mean_target)) + 
  geom_col() + 
  coord_flip() + 
  xlab(NULL) + ylab(NULL) + ggtitle("Average target by profession")
  



```

```{r profession_distribution}

train_set %>% 
  count(profession) %>% 
  ggplot(aes(x = reorder(profession,n), y = n)) + 
  geom_col() + 
  coord_flip() + 
  xlab(NULL) + ylab(NULL) + ggtitle("Profession distribution")
  

```

We can see that some professions are relatively rare so we may try to drop
them all in one category (other)


# Summary
* Reduce skewness by Yeo-Johnson (expfsq, exppsq)
* Reverse OHE in profession and try (target) embeddings





